# ğŸšŒ BUS_Prediction

This project predicts bus status using various deep learning models, including LSTM and GRU.

## ğŸ“‹ Project Overview

The project uses historical bus status data to predict future bus statuses. Each model is trained on sequences of bus status observations and learns to predict the next status in the sequence.

## ğŸ“ Project Structure

- **`train_comparison.py`**: ğŸ‹ï¸ Main training script that processes a single Excel file (`dataset/Status_90.xlsx`), trains all models sequentially, and saves individual model results to Excel files.
- **`inference.py`**: ğŸ” Script for running inference on trained models.
- **`plot_model_history.py`**: ğŸ“Š Visualization script to generate performance plots from the result Excel files.
- **`dataset/`**: ğŸ“‚ Directory containing input data (Excel files).
- **`result_*.xlsx`**: ğŸ“ˆ Individual Excel files containing epoch-by-epoch performance metrics for each model (e.g., `result_LSTM.xlsx`, `result_GRU.xlsx`).
- **`*.pth`**: ğŸ’¾ Saved model checkpoints (e.g., `lstm_90_0_æ˜ŸæœŸä¸‰_ç¬¬2ç­.pth`).
- **`requirements.txt`**: ğŸ“¦ Python dependencies.

## âœ¨ Key Features

- **ğŸ¯ Single File Input**: Processes one Excel file at a time (configurable in the script).
- **ğŸ“… Time-Based Splitting**: Training and test data are split based on dates (90/10 split).
- **ğŸ”¢ Sequence Padding**: Sequences shorter than the fixed length (27) are padded with -1.
- **ğŸ”„ Cumulative Prediction**: 
    - First bus predicts second bus ğŸšŒ â†’ ğŸšŒ
    - First + Second buses predict third bus ğŸšŒğŸšŒ â†’ ğŸšŒ
    - And so on...
- **ğŸ“„ Individual Model Results**: Each model's training history is saved to a separate Excel file.

## ğŸŒŸ Environment Setup

Ensure you have Python and the required libraries installed. It is recommended to use a virtual environment.

## ğŸ“¦ Installation

Install the required dependencies:

```bash
pip install -r requirements.txt
```

## ğŸš€ Usage

### 1. ğŸ‹ï¸ Train Models

To train all models (LSTM, GRU, Transformer, BERT) on the configured dataset:

```bash
python train_comparison.py
```

**What happens:**
- ğŸ“¥ Loads data from `dataset/Status_90.xlsx`.
- âœ‚ï¸ Splits data by date (first 90% for training, last 10% for testing).
- ğŸ“ Trains each model for up to 1000 epochs with early stopping (patience 150).
- ğŸ’¾ Saves model checkpoints (e.g., `lstm_90_0_æ˜ŸæœŸä¸‰_ç¬¬2ç­.pth`).
- ğŸ“Š Saves training history to individual Excel files (e.g., `result_LSTM.xlsx`).

### 2. ğŸ” Run Inference

To run inference on a trained model:

Modify `inference.py` or use the function in `train_comparison.py`.

### 3. ğŸ“Š Visualize Results

To generate performance comparison plots from the result files:

```bash
python plot_model_history.py
```

This creates `model_comparison_plot.png` showing Train/Test Loss and Accuracy for all models across epochs. ğŸ“ˆ

## âš™ï¸ Configuration

You can modify the following parameters in `train_comparison.py`:

- **`DATA_FILE`**: ğŸ“‚ Path to the input Excel file (default: `'./dataset/Status_90.xlsx'`)
- **`SEQUENCE_LENGTH`**: ğŸ”¢ Fixed sequence length with padding (default: `27`)
- **`BATCH_SIZE`**: ğŸ“¦ Training batch size (default: `32`)
- **`HIDDEN_SIZE`**: ğŸ§  Hidden layer size for LSTM/GRU (default: `256`)
- **`NUM_LAYERS`**: ğŸ—ï¸ Number of layers (default: `3`)
- **`LEARNING_RATE`**: ğŸ“‰ Learning rate (default: `0.001`)
- **`NUM_EPOCHS`**: ğŸ”„ Number of training epochs (default: `1000`)
- **`PATIENCE`**: â³ Early stopping patience (default: `150`)
- **`TRAIN_SPLIT_RATIO`**: âœ‚ï¸ Ratio for train/test split by date (default: `0.9`)
- **`TARGET_BUS`**: ğŸšŒ Target bus to filter (default: `"ç¬¬2ç­"`)
- **`DAY`**: ğŸ“… Day filter (default: `"æ˜ŸæœŸä¸‰"`)

## ğŸ¤– Models Implemented

- **LSTM** (Long Short-Term Memory) ğŸ§ 
- **GRU** (Gated Recurrent Unit) ğŸ”„
- **Transformer** (Encoder-only with positional encoding) ğŸ¤–
- **BERT** (Custom implementation using Hugging Face configuration) ğŸ“š
